{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction de Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import yake\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraire les mots clés d'un document avec Yake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/LIAAD/yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantier l'extracteur de mots clés\n",
    "kw_extractor = yake.KeywordExtractor(lan=\"fr\", top=50)\n",
    "kw_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path = \"../data/txt/\"\n",
    "files = os.listdir(data_path)\n",
    " \n",
    "# Valeur de l'année à rechercher\n",
    "year = \"1910\"\n",
    " \n",
    "# Filtre des fichiers correspondant à l'année spécifiée\n",
    "year_files = [file for file in files]\n",
    " \n",
    "# Impression du nombre de fichiers trouvés et les noms de ces fichiers\n",
    "print(\"Les fichiers correspondants :\")\n",
    "for file in year_files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimer le nombre de fichiers identifiés\n",
    "len(year_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les cents fichiers\n",
    "year_files[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changer le nom de la variable \n",
    "this_file = year_files\n",
    "this_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de liste pour stocker les textes\n",
    "texts = []\n",
    "\n",
    "# Parcourez la liste des noms de fichiers\n",
    "for this_file in this_file:\n",
    "    file_path = os.path.join(data_path, this_file)\n",
    "    \n",
    "    # Vérifiez si le fichier existe\n",
    "    if os.path.exists(file_path):\n",
    "        # Lisez le contenu du fichier et stockez-le dans la liste\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            file_text = file.read()\n",
    "            texts.append(file_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire les mots clés de ce texte\n",
    "keywords = kw_extractor.extract_keywords(file_text)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ne garder que les bigrammes\n",
    "kept = []\n",
    "for kw, score in keywords:\n",
    "    words = kw.split()\n",
    "    if len(words) == 2:\n",
    "        kept.append(kw)\n",
    "kept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faire la même opération sur tous les documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in sorted(year_files):\n",
    "    text = open(os.path.join(data_path, f), 'r', encoding=\"utf-8\").read()\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "    kept = []\n",
    "    for kw, score in keywords:\n",
    "        words = kw.split()\n",
    "        if len(words) == 2:\n",
    "            kept.append(kw)\n",
    "    print(f\"{f} mentions these keywords: {', '.join(kept)}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NUAGES DE MOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words(\"french\")\n",
    "sw += [\"les\", \"plus\", \"cette\", \"fait\", \"faire\", \"être\", \"deux\", \"comme\", \"dont\", \"tout\",\n",
    "       \"ils\", \"bien\", \"sans\", \"peut\", \"tous\", \"après\", \"ainsi\", \"donc\", \"cet\", \"sous\",\n",
    "       \"celle\", \"entre\", \"encore\", \"toutes\", \"pendant\", \"moins\", \"dire\", \"cela\", \"non\",\n",
    "       \"faut\", \"trois\", \"aussi\", \"dit\", \"avoir\", \"doit\", \"contre\", \"depuis\", \"autres\",\n",
    "       \"van\", \"het\", \"autre\", \"jusqu\", \"ville\", \"rossel\", \"dem\", \"la\", \"jusqu'à\", \"trois\", \n",
    "       \"voilà\",\"le\", \"du\", \"afin,\" \"après\", \"aucun\", \"aujourd\", \"assez\", \"allons\", \"abord\"\n",
    "       \"tellment\", \"telle\", \"tienne\", \"vif\", \"va\", \"via\", \"vers\", \"vives\", \"vous\", \"encore\", \"tic\"\n",
    "       \"tes\", \"te\", \"votre\", \"surtout\", \"tac\", \"sur\", \"vous-memes\", \"treize\",\"sur\", \"un\", \"notre\",\n",
    "        \"vais\", \"trop\", \"sujet\",\"toc\",\"vont\",\"toi-meme\", \"tien\", \"suivant\", \"mienne\", \"tels\"\n",
    "        \"devant\", \"téléphone\", \"devant\", \"acheter\",\"vol\", \"achat\", \"libre\", \"offre\", \"bon\"\n",
    "        \"pr\", \"ecr\",\"une\", \"encore\", \"suite\", \"ceux\", \"celle\", \"celui\", \"celui-ci\", \"mais\"\n",
    "        \"ser\", \"mai\", \"display\", \"ou\", \"dès\", \"ref\", \"quelques\", \"très\", \"vue\", \"voir\"\n",
    "        \"une\", \"lot\", \"tél\", \"bon\", \"mme\", \"après\", \"fin\",\"notamment\", \"aussi\", \"les\", \"toutes\",\n",
    "         \"elles\", \"ils\", \"vos\", \"elle\", \"celle-la\", \"tellement\", \"supérieur\", \"grace\", \"pas\", \"quand\",\n",
    "        \"vont\", \"force\", \"forte\", \"egal\", \"plus\", \"celle\", \"deux\", \"achat\", \"corpus\", \"nom\", \"lorsque\",\n",
    "        \"pendant que\", \"alors que\", \"eut\", \"eurent\", \"chemin\", \"ensemble\", \"est\", \"était\", \"étaient\",\n",
    "        \"vivre\", \"vit\", \"allons\", \"sur\", \"avant\", \"mal\", \"a\", \"avait\", \"à\", \"ici\", \"parce\", \"parce que\", \n",
    "       \"hier\", \"pris\", \"déjà\", \"dix\", \"point\", \"nouveau\", \"rue\", \"reçu\", \"toute\", \"tout\", \"ans\",\n",
    "       \"raison\", \"mis\", \"art\", \"huit\", \"cas\", \"tant\", \"mis\", \"pris\", \"fort\", \"fit\", \"dos\", \"appel\", \n",
    "        \"ensuite\", \"voie\", \"samedi\", \"tete\", \"vie\", \"ira\", \"peu\", \"heure\", \"fils\", \"rue\", \"midi\", \n",
    "       \"jamais\", \"place\", \"cinq\", \"raison\", \"dan\", \"dans\", \"car\", \"ensuite\", \"oui\", \"on\", \"ont\", \"effet\",\n",
    "       \"soir\", \"dos\", \"hui\", \"midi\", \"dos\", \"en\", \"samedi\", \"peine\", \"alors\", \"oui\", \"fer\", \"enfin\", \n",
    "       \"titre\", \"voir\", \"tant\", \"tant que\", \"oui\", \"jeudi\", \"ici\", \"chez\", \"mois\", \"cas\", \"temps\"]\n",
    "sw = set(sw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(sw)} stopwords:\\n {sorted(sw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choisir une année\n",
    "year = 1910"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lister les fichiers de cette année\n",
    "data_path = '../data'\n",
    "txt_path = '../data/txt'\n",
    "txts = [f for f in os.listdir(txt_path) if os.path.isfile(os.path.join(txt_path, f)) and str(year) in f]\n",
    "len(txts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stocker le contenu de ces fichiers dans une liste\n",
    "content_list = []\n",
    "for txt in txts:\n",
    "    with open(os.path.join(txt_path, txt), 'r', encoding='utf-8') as f:\n",
    "        content_list.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter le nombre d'éléments donc fichiers) dans la liste\n",
    "len(content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimer tous les caractères du contenu dans les fichiers\n",
    "for text in content_list:\n",
    "    for char in text:\n",
    "        print(char, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ecrire tout le contenu dans un fichier temporaire\n",
    "temp_path = '../data/tmp'\n",
    "if not os.path.exists(temp_path):\n",
    "    os.mkdir(temp_path)\n",
    "with open(os.path.join(temp_path, f'{year}.txt'), 'w', encoding='utf-8') as f:\n",
    "    f.write(' '.join(content_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimer le contenu du fichier et constater les \"déchets\"\n",
    "with open(os.path.join(temp_path, f'{year}.txt'), 'r', encoding='utf-8') as f:\n",
    "    before = f.read()\n",
    "\n",
    "before[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(year, folder=None):\n",
    "    if folder is None:\n",
    "        input_path = f\"{year}.txt\"\n",
    "        output_path = f\"{year}_clean.txt\"\n",
    "    else:\n",
    "        input_path = f\"{folder}/{year}.txt\"\n",
    "        output_path = f\"{folder}/{year}_clean.txt\"\n",
    "    output = open(output_path, \"w\", encoding='utf-8')\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        words = nltk.wordpunct_tokenize(text)\n",
    "        kept = [w.upper() for w in words if len(w) > 2 and w.isalpha() and w.lower() not in sw]\n",
    "        kept_string = \" \".join(kept)\n",
    "        output.write(kept_string)\n",
    "    return f'Output has been written in {output_path}!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(year, folder=temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier le résultat\n",
    "with open(os.path.join(temp_path, f'{year}_clean.txt'), 'r', encoding='utf-8') as f:\n",
    "    after = f.read()\n",
    "\n",
    "after[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = Counter(after.split())\n",
    "print(frequencies.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = WordCloud(width=2000, height=1000, background_color='white').generate_from_frequencies(frequencies)\n",
    "cloud.to_file(os.path.join(temp_path, f\"{year}.png\"))\n",
    "Image(filename=os.path.join(temp_path, f\"{year}.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RECONNAISSANCE D'ENTITE NOMMEES AVEC SPACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import sys\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=100000\n",
    "text = open(\"../data/all.txt\", encoding='utf-8').read()[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Traiter le texte\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter les entités\n",
    "people = defaultdict(int)\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"PER\" and len(ent.text) > 3:\n",
    "        people[ent.text] += 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_people = sorted(people.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "for person, freq in sorted_people[:30]:\n",
    "    print(f\"{person} apparait {freq} fois dans le corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afficher les Lieux et Organisations les plus mentionnées dans le Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse du texte  avec SpaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extraction et count des lieux (LOC) et les organisations (ORG)\n",
    "locations = {}\n",
    "organizations = {}\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"LOC\":\n",
    "        locations[ent.text] = locations.get(ent.text, 0) + 1\n",
    "    elif ent.label_ == \"ORG\":\n",
    "        organizations[ent.text] = organizations.get(ent.text, 0) + 1\n",
    "\n",
    "# Trier et afficher les lieux les plus mentionnés\n",
    "sorted_locations = sorted(locations.items(), key=lambda kv: kv[1], reverse=True)\n",
    "print(\"\\nLieux les plus mentionnés :\")\n",
    "for location, freq in sorted_locations[:50]:\n",
    "    print(f\"{location} apparait {freq} fois dans le corpus\")\n",
    "\n",
    "# Trier et afficher les organisations les plus mentionnées\n",
    "sorted_organizations = sorted(organizations.items(), key=lambda kv: kv[1], reverse=True)\n",
    "print(\"\\nOrganisations les plus mentionnées :\")\n",
    "for organization, freq in sorted_organizations[:50]:\n",
    "    print(f\"{organization} apparait {freq} fois dans le corpus\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('tac_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1932ab1d169b4769d1550e799423b6477588e745f266d79d9004c136c81607e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
