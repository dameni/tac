{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction de Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import yake\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from IPython.display import Image\n",
    "import sys\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraire les mots clés d'un document avec Yake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/LIAAD/yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantier l'extracteur de mots clés\n",
    "kw_extractor = yake.KeywordExtractor(lan=\"fr\", top=50)\n",
    "kw_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path = \"../data/txt/\"\n",
    "\n",
    "files = [f for f in sorted(os.listdir(data_path)) if f.endswith('.txt')]\n",
    "len(files)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les fichiers\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changer le nom de la variable \n",
    "this_file = files\n",
    "this_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de liste pour stocker les textes\n",
    "texts = []\n",
    "\n",
    "# Parcourez la liste des noms de fichiers\n",
    "for this_file in this_file:\n",
    "    file_path = os.path.join(data_path, this_file)\n",
    "    \n",
    "    # Vérifiez si le fichier existe\n",
    "    if os.path.exists(file_path):\n",
    "        # Lisez le contenu du fichier et stockez-le dans la liste\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            file_text = file.read()\n",
    "            texts.append(file_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire les mots clés de ce texte\n",
    "keywords = kw_extractor.extract_keywords(file_text)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ne garder que les bigrammes\n",
    "kept = []\n",
    "for kw, score in keywords:\n",
    "    words = kw.split()\n",
    "    if len(words) == 2:\n",
    "        kept.append(kw)\n",
    "kept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faire la même opération sur tous les documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in sorted(files):\n",
    "    text = open(os.path.join(data_path, f), 'r', encoding=\"utf-8\").read()\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "    kept = []\n",
    "    for kw, score in keywords:\n",
    "        words = kw.split()\n",
    "        if len(words) == 2:\n",
    "            kept.append(kw)\n",
    "    print(f\"{f} mentions these keywords: {', '.join(kept)}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NUAGES DE MOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words(\"french\")\n",
    "sw += [\"les\", \"plus\", \"cette\", \"fait\", \"faire\", \"être\", \"deux\", \"comme\", \"dont\", \"tout\",\n",
    "       \"ils\", \"bien\", \"sans\", \"peut\", \"tous\", \"après\", \"ainsi\", \"donc\", \"cet\", \"sous\",\n",
    "       \"celle\", \"entre\", \"encore\", \"toutes\", \"pendant\", \"moins\", \"dire\", \"cela\", \"non\",\n",
    "       \"faut\", \"trois\", \"aussi\", \"dit\", \"avoir\", \"doit\", \"contre\", \"depuis\", \"autres\",\n",
    "       \"van\", \"het\", \"autre\", \"jusqu\", \"ville\", \"rossel\", \"dem\", \"la\", \"jusqu'à\", \"trois\", \n",
    "       \"voilà\",\"le\", \"du\", \"afin,\" \"après\", \"aucun\", \"aujourd\", \"assez\", \"allons\", \"abord\"\n",
    "       \"tellment\", \"telle\", \"tienne\", \"vif\", \"va\", \"via\", \"vers\", \"vives\", \"vous\", \"encore\", \"tic\"\n",
    "       \"tes\", \"te\", \"votre\", \"surtout\", \"tac\", \"sur\", \"vous-memes\", \"treize\",\"sur\", \"un\", \"notre\",\n",
    "       \"vais\", \"trop\", \"sujet\",\"toc\",\"vont\",\"toi-meme\", \"tien\", \"suivant\", \"mienne\", \"tels\"\n",
    "       \"devant\", \"téléphone\", \"devant\", \"acheter\",\"vol\", \"achat\", \"libre\", \"offre\", \"bon\"\n",
    "       \"pr\", \"ecr\",\"une\", \"encore\", \"suite\", \"ceux\", \"celle\", \"celui\", \"celui-ci\", \"mais\"\n",
    "       \"ser\", \"mai\", \"display\", \"ou\", \"dès\", \"ref\", \"quelques\", \"très\", \"vue\", \"voir\"\n",
    "       \"une\", \"lot\", \"tél\", \"bon\", \"mme\", \"après\", \"fin\",\"notamment\", \"aussi\", \"les\", \"toutes\",\n",
    "       \"elles\", \"ils\", \"vos\", \"elle\", \"celle-la\", \"tellement\", \"supérieur\", \"grace\", \"pas\", \"quand\",\n",
    "       \"vont\", \"force\", \"forte\", \"egal\", \"plus\", \"celle\", \"deux\", \"achat\", \"corpus\", \"nom\", \"lorsque\",\n",
    "       \"pendant que\", \"alors que\", \"eut\", \"eurent\", \"chemin\", \"ensemble\", \"est\", \"était\", \"étaient\",\n",
    "       \"vivre\", \"vit\", \"allons\", \"sur\", \"avant\", \"mal\", \"a\", \"avait\", \"à\", \"ici\", \"parce\", \"parce que\", \n",
    "       \"hier\", \"pris\", \"déjà\", \"dix\", \"point\", \"nouveau\", \"rue\", \"reçu\", \"toute\", \"tout\", \"ans\",\n",
    "       \"raison\", \"mis\", \"art\", \"huit\", \"cas\", \"tant\", \"mis\", \"pris\", \"fort\", \"fit\", \"dos\", \"appel\", \n",
    "       \"ensuite\", \"voie\", \"samedi\", \"tete\", \"vie\", \"ira\", \"peu\", \"heure\", \"fils\", \"rue\", \"midi\", \n",
    "       \"jamais\", \"place\", \"cinq\", \"raison\", \"dan\", \"dans\", \"car\", \"ensuite\", \"oui\", \"on\", \"ont\", \"effet\",\n",
    "       \"soir\", \"dos\", \"hui\", \"midi\", \"dos\", \"en\", \"samedi\", \"peine\", \"alors\", \"oui\", \"fer\", \"enfin\", \n",
    "       \"titre\", \"voir\", \"tant\", \"tant que\", \"oui\", \"jeudi\", \"ici\", \"chez\", \"mois\", \"cas\", \"temps\", \"près\",\n",
    "       \"matin\", \"aucune\", \"haut\", \"dernier\", \"tour\", \"qne\", \"matin\", \"uue\", \"pourra\", \"haut\", \"prix\", \n",
    "       \"donner\", \"jour\", \"nuit\", \"six\", \"faite\", \"reste\", \"main\", \"puis\", \"choses\", \"veut\", \"seul\", \n",
    "       \"moyen\", \"prit\", \"mettre\", \"dernier\", \"aussitot\", \"mieux\", \"cause\", \"chaque\", \"prix\", \"leurs\", \n",
    "       \"fois\", \"mit\", \"seul\", \"matin\", \"bonne\", \"chose\", \"part\", \"lieu\", \"petit\", \"pouvoir\", \n",
    "       \"veut\", \"dooner\", \"rendu\", \"eau\", \"nuit\", \"hotel\", \"jours\", \"septembre\", \"heures\", \"leurs\",\n",
    "       \"fois\", \"charge\", \"ordre\", \"lequel\", \"presque\", \"charge\", \"somme\", \" faits\", \"lorsqu\", \"choses\",\n",
    "       \"dimanche\", \"dessus\", \"travaux\", \"yeux\", \"vient\", \"heures\", \"sou\", \"coup\", \"etc\"]\n",
    "sw = set(sw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(sw)} stopwords:\\n {sorted(sw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter le nombre d'éléments donc fichiers) dans la liste\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimer tous les caractères du contenu dans les fichiers\n",
    "#for text in texts:\n",
    "   # for char in text:\n",
    "        #print(char, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ecrire tout le contenu dans un fichier temporaire\n",
    "temp_path = '../data/tmp'\n",
    "if not os.path.exists(temp_path):\n",
    "    os.mkdir(temp_path)\n",
    "with open(os.path.join(temp_path, f'all_2.txt'), 'w', encoding='utf-8') as f:\n",
    "    f.write(' '.join(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimer le contenu du fichier et constater les \"déchets\"\n",
    "with open(os.path.join(temp_path, f'all_2.txt'), 'r', encoding='utf-8') as f:\n",
    "    before = f.read()\n",
    "\n",
    "before[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(folder=None):\n",
    "    if folder is None:\n",
    "        input_path = f\"all_2.txt\"\n",
    "        output_path = f\"all_2_clean.txt\"\n",
    "    else:\n",
    "        input_path = f\"{folder}/all_2.txt\"\n",
    "        output_path = f\"{folder}/all_2_clean.txt\"\n",
    "    output = open(output_path, \"w\", encoding='utf-8')\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        words = nltk.wordpunct_tokenize(text)\n",
    "        kept = [w.upper() for w in words if len(w) > 2 and w.isalpha() and w.lower() not in sw]\n",
    "        kept_string = \" \".join(kept)\n",
    "        output.write(kept_string)\n",
    "    return f'Output has been written in {output_path}!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(folder=temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier le résultat\n",
    "with open(os.path.join(temp_path, f'all_2_clean.txt'), 'r', encoding='utf-8') as f:\n",
    "    after = f.read()\n",
    "\n",
    "after[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = Counter(after.split())\n",
    "print(frequencies.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = WordCloud(width=2000, height=1000, background_color='white').generate_from_frequencies(frequencies)\n",
    "cloud.to_file(os.path.join(temp_path, f\"all_2_clean.png\"))\n",
    "Image(filename=os.path.join(temp_path, f\"all_2_clean.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RECONNAISSANCE D'ENTITE NOMMEES AVEC SPACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import sys\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=100000\n",
    "text = open(\"../data/all.txt\", encoding='utf-8').read()[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Traiter le texte\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter les entités\n",
    "people = defaultdict(int)\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"PER\" and len(ent.text) > 3:\n",
    "        people[ent.text] += 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_people = sorted(people.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "for person, freq in sorted_people[:30]:\n",
    "    print(f\"{person} apparait {freq} fois dans le corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afficher les Lieux et Organisations les plus mentionnées dans le Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse du texte  avec SpaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extraction et count des lieux (LOC) et les organisations (ORG)\n",
    "locations = {}\n",
    "organizations = {}\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"LOC\":\n",
    "        locations[ent.text] = locations.get(ent.text, 0) + 1\n",
    "    elif ent.label_ == \"ORG\":\n",
    "        organizations[ent.text] = organizations.get(ent.text, 0) + 1\n",
    "\n",
    "# Trier et afficher les lieux les plus mentionnés\n",
    "sorted_locations = sorted(locations.items(), key=lambda kv: kv[1], reverse=True)\n",
    "print(\"\\nLieux les plus mentionnés :\")\n",
    "for location, freq in sorted_locations[:50]:\n",
    "    print(f\"{location} apparait {freq} fois dans le corpus\")\n",
    "\n",
    "# Trier et afficher les organisations les plus mentionnées\n",
    "sorted_organizations = sorted(organizations.items(), key=lambda kv: kv[1], reverse=True)\n",
    "print(\"\\nOrganisations les plus mentionnées :\")\n",
    "for organization, freq in sorted_organizations[:50]:\n",
    "    print(f\"{organization} apparait {freq} fois dans le corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis avec Textblob-FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from textblob import Blobber\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = Blobber(pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "\n",
    "def get_sentiment(input_text):\n",
    "    blob = tb(input_text)\n",
    "    polarity, subjectivity = blob.sentiment\n",
    "    polarity_perc = f\"{100*abs(polarity):.0f}\"\n",
    "    subjectivity_perc = f\"{100*subjectivity:.0f}\"\n",
    "    if polarity > 0:\n",
    "        polarity_str = f\"{polarity_perc}% positive\"\n",
    "    elif polarity < 0:\n",
    "        polarity_str = f\"{polarity_perc}% negative\"\n",
    "    else:\n",
    "        polarity_str = \"neutral\"\n",
    "    if subjectivity > 0:\n",
    "        subjectivity_str = f\"{subjectivity}% subjective\"\n",
    "    else:\n",
    "        subjectivity_str = \"perfectly objective\"\n",
    "    print(f\"This text is {polarity_str} and {subjectivity_str}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse du sentiment dans 10 phrases comportant le mot \"Femme\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\" Pour les Travailleuses, contre le Féminisme lacs communistes ne voient on principe aucun intérêt à la conquête du bulletin de vote par « lu Française a>, pus plus que pur « le i' raneais », par • ia femme a> pas plus que pour l'homme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"quant aux rapports juridiques, ils etablissent sur l'égalité de tous les citoyens sans exception, c'est à dire de tous les habitants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Et nous combattons le féminisme à cause de ces illusions et préjugés néfastes, nous nous efforçons de soustraire les travailleuses à son influence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"quand les femmes françaises auront le droit de voter, on s'étonnera des obstacles qu'elles ont rencontrés pour l'obtenir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Quant au point de savoir si le parlement de la bourgeoisie française accordera bientôt le droit de vote aux femmes, il nous paraît qu'il l'accordera au moment où il jugera que le vote féminin est nécessaire pour renforcer la majorité bourgeoise légale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"En Belgique, plus de la moitié de ta population est composée de femmes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"La Journée Internationale des Femmes du 8 mars, qui fut instituée à l'initiative de ce modèle de la femme révolutionnaire qu'était notre grande et vénérée Clara Zetldn, doit nous être une occasion nouvelle de poser et de résoudre le problème de la conquête des femmes travailleuses au Communisme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"La place de la femme est au foyer !, a déclaré Hitler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Le libre accès à toutes les formes d'enseignement et l'égalité dans les conditions d'admission à tous les Etablissements d'enseignement : primaire, secondaire, supérieur, artistique, technique, etc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\" A ohaque fois que les élections approchent, l'on commence à s'agiter sur le problème du droit de vote des femmes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse du sentiment dans 10 phrases comportant le mot \"Droit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Un grand nombre de femmes suivant les partis socialistes crurent que petit à petit par la lutte parlementaire, par la voie des réformes, elles parviendront à obtenir l'égalité des droits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"L'égalité des droits politiques\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Le droit à soustraire leurs enfants à toute éducation militariste\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Même les partis les moins partisans de ce droit de vote en période habituelle, sachant combien la' femme a pris aujourd'hui une place incontestée dans tous les domaines de la vie sociale, ne peuvent en période électorale ignorer ce problème, résolu depuis bien longtemps déjà, et en faveur du droit de vote des femmes, dans de nombreux pays démocratiques\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Le parti communiste qui a toujours eu une position claire sur ce problème, vient de faire déposer sur le bureau de la Chambre par les députés Alice Degeer, Bosson et Relecom, un projet de loi attribuant purement et simplement droit de vote aux femmes au même titre qu'aux hommes, projet qui ne se contente pas seulement d'être clair mais qui rapœ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Partisans du droit des femmes, les libéraux pensent qu il faut donner à la femme une préparation générale qui lui permette de se guider elle-même dans la vie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Le droit de suffrage féminin à se de- pré de l'électorat n'est d'ailleurs pas même en discussion, mais la Chambre à commencé à examiner l'électanat féminin à la province\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Parallèlement à cela, les femmes de la petite bourgeoisie forcent les portes des universités et des écoles, les suffragettes anglaises mènent une lutte courageuse pour le droit de vote des femmes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Après la guerre,, certains pays ont accordé le droit de vote aux femmes, par exemple l'Allemagne, l'Angleterre, la Suède\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"s'agit d'établir ici l'opportunité du droit de vote des femmes pour la province, pour faire ainsi un pas de plus vers le suffrage réellement universel, qui n'existe pas encore avec notre législation actuelle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering des documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import string\n",
    "import sys\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "from timeout_decorator import timeout, TimeoutError\n",
    "from threading import Timer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectoriser les documents à l'aide de TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vectoriser les documents à l'aide de TF-IDF\n",
    "# Création d'une fonction de pré-traitement\n",
    "def preprocessing(text, stem=True):\n",
    "    \"\"\" Tokenize text and remove punctuation \"\"\"\n",
    "    text = text.translate(string.punctuation)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instancier le modèle TF-IDF avec ses arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=preprocessing,\n",
    "    stop_words=stopwords.words('french'),\n",
    "    max_df=0.5,\n",
    "    min_df=0.1,\n",
    "    lowercase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construire la matrice de vecteurs à l'aide de la fonction `fit_transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectors = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Détail de la matrice\n",
    "tfidf_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imprimer le vecteur tf-IDF du premier document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(\n",
    "    tfidf_vectors[0].toarray()[0],\n",
    "    index=vectorizer.get_feature_names_out()\n",
    "    ).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprendre les vecteurs et leurs \"distances\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine([1, 2, 3], [1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine([1, 2, 3], [1, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine([1, 2, 3], [2, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests sur nos documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_array = tfidf_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vecteur du document 0\n",
    "tfidf_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vecteur du document 1\n",
    "tfidf_array[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(tfidf_array[0], tfidf_array[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# définir un nombres de clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appliquer un algorithme de clustering sur les vecteurs TF-IDF des documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLUSTERS = 2\n",
    "# définir le nombre de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_model = KMeans(n_clusters=N_CLUSTERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appliquer le clustering à l'aide de la fonction `fit_predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = km_model.fit_predict(tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = collections.defaultdict(list)\n",
    "\n",
    "for idx, label in enumerate(clusters):\n",
    "    clustering[label].append(files[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(dict(clustering))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture du  contenu des clusters avec une limite de temps\n",
    "def read_file_content(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "# gestion  du  délai d'exécution\n",
    "def timeout_handler():\n",
    "    raise TimeoutError(\"Function execution timed out\")\n",
    "\n",
    "# Exploration des données brutes de chaque cluster\n",
    "for cluster_label, files_in_cluster in clustering.items():\n",
    "    print(f\"Cluster {cluster_label} - Raw Data:\")\n",
    "    #pprint(files_in_cluster)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Stocke du contenu de ces fichiers dans une liste\n",
    "    content_list = []\n",
    "    for txt in files_in_cluster:\n",
    "        timer = Timer(5, timeout_handler)  # Définissez le délai en secondes\n",
    "        try:\n",
    "            timer.start()\n",
    "            content = read_file_content(os.path.join(data_path, txt))\n",
    "            content_list.append(content)\n",
    "        except TimeoutError:\n",
    "            print(f\"Reading {txt} took too long. Skipping.\")\n",
    "            continue\n",
    "        finally:\n",
    "            timer.cancel()\n",
    "\n",
    "    # Afficher le contenu de chaque texte dans le cluster\n",
    "    for idx, text in enumerate(content_list):\n",
    "        print(f\"Text {idx + 1} in Cluster {cluster_label}:\")\n",
    "        print(text)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualiser les clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "reduced_vectors = pca.fit_transform(tfidf_vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_vectors[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génerer le plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = reduced_vectors[:, 0]\n",
    "y_axis = reduced_vectors[:, 1]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "scatter = plt.scatter(x_axis, y_axis, s=100, c=clusters)\n",
    "\n",
    "# Ajouter les centroïdes\n",
    "centroids = pca.transform(km_model.cluster_centers_)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],  marker = \"x\", s=100, linewidths = 2, color='black')\n",
    "\n",
    "# Ajouter la légende\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=set(clusters), title=\"Clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation en phrases du corpus complet et création d'un nouveau dossier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from unidecode import unidecode\n",
    "from scipy.spatial.distance import cosine\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = \"../data/tmp/all_2.txt\"\n",
    "outfile = \"../data/sents.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    \"\"\"Tokenize and Lemmatize sentences\"\"\"\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename, encoding='utf-8', errors=\"backslashreplace\"):\n",
    "            yield [unidecode(w.lower()) for w in wordpunct_tokenize(line)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outfile, 'w', encoding=\"utf-8\") as output:\n",
    "    with open(infile, encoding=\"utf-8\", errors=\"backslashreplace\") as f:\n",
    "        content = f.readlines()\n",
    "        content=content[:LIMIT] if LIMIT is not None else content\n",
    "        n_lines = len(content)\n",
    "        \n",
    "        for i, line in enumerate(content):\n",
    "            if i % 100 == 0:\n",
    "                print(f'processing line {i}/{n_lines}')\n",
    "            sentences = sent_tokenize(line)\n",
    "            for sent in sentences:\n",
    "                output.write(sent + \"\\n\")\n",
    "print(\"Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    \"\"\"Tokenize and Lemmatize sentences\"\"\"\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename, encoding='utf-8', errors=\"backslashreplace\"):\n",
    "            yield [unidecode(w.lower()) for w in wordpunct_tokenize(line)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = f\"../data/sents.txt\"\n",
    "sentences = MySentences(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phrases = Phrases(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(bigram_phrases.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bigram_phrases.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_ = list(bigram_phrases.vocab.keys())[144]\n",
    "print(key_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phrases.vocab[key_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phraser = Phraser(phrases_model=bigram_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_phrases = Phrases(bigram_phraser[sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_phraser = Phraser(phrases_model=trigram_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(trigram_phraser[bigram_phraser[sentences]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Word2Vec(\n",
    "    corpus, # On passe le corpus de ngrams que nous venons de créer\n",
    "    vector_size=32, # Le nombre de dimensions dans lesquelles le contexte des mots devra être réduit, aka. vector_size\n",
    "    window=5, # La taille du \"contexte\", ici 5 mots avant et après le mot observé\n",
    "    min_count=5, # On ignore les mots qui n'apparaissent pas au moins 5 fois dans le corpus\n",
    "    workers=4, # Permet de paralléliser l'entraînement du modèle en 4 threads\n",
    "    epochs=5 # Nombre d'itérations du réseau de neurones sur le jeu de données pour ajuster les paramètres avec la descente de gradient, aka. epochs.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = f\"../data/newspapers.model\"\n",
    "model.save(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"../data/newspapers.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv[\"femme\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv[\"droit\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculer la similarité entre deux termes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"femme\", \"droit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"droit\", \"gouvernement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"femme\", \"gouvernement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"femme\", \"homme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"vote\", \"femme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"femme\", \"tribunal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"femme\", \"travail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chercher les mots les plus proches d'un terme donné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"femme\", topn=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"droit\", topn=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"homme\", topn=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# on entraine notre modèle avec de nouvelles paramètre en mettant le \n",
    "##window=8\n",
    "##min_count=9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Word2Vec(\n",
    "    corpus, # On passe le corpus de ngrams que nous venons de créer\n",
    "    vector_size=70, # Le nombre de dimensions dans lesquelles le contexte des mots devra être réduit, aka. vector_size\n",
    "    window=8, # La taille du \"contexte\", ici 7 mots avant et après le mot observé\n",
    "    min_count=9, # On ignore les mots qui n'apparaissent pas au moins 5 fois dans le corpus\n",
    "    workers=4, # Permet de paralléliser l'entraînement du modèle en 4 threads\n",
    "    epochs=5 # Nombre d'itérations du réseau de neurones sur le jeu de données pour ajuster les paramètres avec la descente de gradient, aka. epochs.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv[\"femme\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv[\"droit\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on calcule de nouveau la similarité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"femme\", \"droit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"droit\", \"gouvernement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"femme\", \"gouvernement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"vote\", \"femme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"femme\", \"tribunal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chercher les mots les plus proches d'un terme donné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"femme\", topn=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"droit\", topn=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"homme\", topn=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## on entraine encore le modèle avec comme la taille de window=10 et le min_count=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Word2Vec(\n",
    "    corpus, # On passe le corpus de ngrams que nous venons de créer\n",
    "    vector_size=32, # Le nombre de dimensions dans lesquelles le contexte des mots devra être réduit, aka. vector_size\n",
    "    window=10, # La taille du \"contexte\", ici 7 mots avant et après le mot observé\n",
    "    min_count=15, # On ignore les mots qui n'apparaissent pas au moins 5 fois dans le corpus\n",
    "    workers=4, # Permet de paralléliser l'entraînement du modèle en 4 threads\n",
    "    epochs=5 # Nombre d'itérations du réseau de neurones sur le jeu de données pour ajuster les paramètres avec la descente de gradient, aka. epochs.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv[\"femme\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv[\"droit\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on calcule de nouveau la similarité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"femme\", \"droit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"femme\", \"gouvernement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"droit\", \"gouvernement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"vote\", \"femme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"femme\", \"tribunal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chercher les mots les plus proches d'un terme donné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"femme\", topn=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"droit\", topn=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"homme\", topn=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('tac_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1932ab1d169b4769d1550e799423b6477588e745f266d79d9004c136c81607e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
